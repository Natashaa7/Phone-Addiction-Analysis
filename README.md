# ğŸ“± Phone Addiction Analysis

## ğŸ“Œ Overview

Ever wondered how phone usage affects stress, sleep, or school performance?

This project analyzes **teen smartphone addiction** patterns using data science and machine learning. We explore the data, test multiple models, and pick the best one to **predict addiction risk**. It analyzes smartphone addiction patterns in teenagers, identifying behavioral and demographic factors that contribute to addiction. The workflow involves **data preprocessing, exploratory data analysis, feature selection, model comparison, and inference**. The project uses multiple machine learning models to classify users into addiction risk categories and selects the best-performing model (Gradient Boosting).

## ğŸ“Š Dataset

This project uses the [**Teen Phone Addiction Dataset**](https://www.kaggle.com/datasets/sumedh1507/teen-phone-addiction?utm_source=chatgpt.com) from Kaggle.

### ğŸ“ Dataset Overview:

- **Rows:** 300+ (teens surveyed)
- **Columns:** 15 (demographic, behavioral, and academic features)
- **Key Features:**
    - `Gender` â€“ Male/Female
    - `Age` â€“ Age of the teen
    - `Sleep Duration` â€“ Average hours of sleep per night
    - `Study Hours` â€“ Average study time per day
    - `Screen Time` â€“ Daily smartphone usage in hours
    - `Social Media Usage` â€“ Social media usage (hours)
    - `GPA` â€“ Academic performance indicator
    - `Addiction Level` â€“ Target label (Low, Medium, High)

### ğŸ“¥ How to Get the Dataset:

1. Download from Kaggle: [Teen Phone Addiction Dataset]([https://www.kaggle.com/datasets/sumedh1507/teen-phone-addiction])
2. Place the file `teen_phone_addiction_dataset.csv` inside the **`notebooks/`** folder of this repository.
   
## ğŸ—‚ Repository Structure 
```Phone-Addiction-Analysis/
â”‚
â”œâ”€ notebooks/
â”‚   â”œâ”€ EDA.ipynb                  # Exploratory Data Analysis
â”‚   â”œâ”€ Data-Preprocessing.ipynb   # Data cleaning and encoding
â”‚   â”œâ”€ Feature-Selection.ipynb    # Feature importance and selection
â”‚   â”œâ”€ Base-model.ipynb           # Baseline models
â”‚   â”œâ”€ Comparison.ipynb           # Model performance comparison
â”‚   â”œâ”€ Logistic-Regression.ipynb  # Logistic Regression model
â”‚   â”œâ”€ Random-Forest.ipynb        # Random Forest model
â”‚   â”œâ”€ Gradient-Boosting.ipynb    # Gradient Boosting model
â”‚   â”œâ”€ XGBoost.ipynb              # XGBoost model
â”‚   â”œâ”€ Best-Model(Gradient-Boosting).ipynb  # Retrain best model and save
â”‚   â”œâ”€ Infererence-best-model.ipynb  # Predictions using saved model
â”‚
â”œâ”€ source/model/                   # Saved ML models using joblib
â”œâ”€ teen_phone_addiction_dataset.csv
â”œâ”€ [app.py](http://app.py/)                          # Web app or API entry point
â”œâ”€ [inference.py](http://inference.py/)                    # Script for running inference
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â”œâ”€ [README.md](http://readme.md/) 
```

## ğŸ§° Tools & Libraries

- **Programming Language:** Python
- **Data Analysis & Visualization:** Pandas, NumPy, Matplotlib, Seaborn
- **Machine Learning:** Scikit-learn, XGBoost, Imbalanced-learn (SMOTE)
- **Serialization:** Joblib (for saving models)
- **Notebook Interface:** Jupyter Notebook
- **Deployment:** FastAPI, Swagger UI

---

## ğŸ“ Workflow & Notebooks

### 1. **EDA.ipynb**

- Performs **Exploratory Data Analysis**
- Visualizes distributions, correlations, and patterns in the dataset
- Identifies trends such as high phone usage before bedtime, app usage patterns, and their effect on stress or academic performance

### 2. **Data-Preprocessing.ipynb**

- Handles **missing values**
- Encodes categorical variables
- Normalizes/standardizes features if required
- Prepares data for modeling

### 3. **Feature-Selection.ipynb**

- Computes **feature importance** using multiple methods
- Reduces dimensionality by removing low-impact features
- Ensures that the final feature set improves model performance

### 4. **Base-model.ipynb**

- Trains **baseline models** to get a performance benchmark
- Simple initial implementations of classifiers

### 5. **Logistic-Regression.ipynb, Random-Forest.ipynb, Gradient-Boosting.ipynb, XGBoost.ipynb**

- Train and evaluate each model **individually**
- Include **line-by-line explanations**
- Output model metrics and visualizations (e.g., confusion matrix heatmaps)
  
### 6. **Comparison.ipynb**

- Implements **four ensemble methods and Logistic Regression**:
    - Logistic Regression
    - Random Forest
    - Gradient Boosting
    - XGBoost
- Compares models using metrics such as accuracy, confusion matrix, and balanced accuracy

### 7. **Best-Model(Gradient-Boosting).ipynb**

- Chooses **Gradient Boosting** as the best performing model
- Retrains it on the full training dataset
- Saves the trained model using **joblib** for future inference

### 8. **Infererence-best-model.ipynb**

- Loads the **saved Gradient Boosting model**
- Demonstrates how to make predictions on new/unseen data
- Explains each step of inference clearly

## ğŸ’¾ Saving & Loading Models

- Models are saved in **`source/model`** using `joblib` for efficient serialization:

```python
import joblib

# Save model
joblib.dump(model, "source/model/gradient_boosting_model.joblib")

# Load model
model = joblib.load("source/model/gradient_boosting_model.joblib")

```

- Both **resampled datasets** (from SMOTE) and trained models are saved for reproducibility.

---

## ğŸš€ How to Run

1. Clone the repository:

```bash
git clone https://github.com/Natashaa7/Phone-Addiction-Analysis.git
cd Phone-Addiction-Analysis

```

1. Install dependencies:

```bash
pip install -r requirements.txt

```

1. Run notebooks in this order for full workflow:
    1. `EDA.ipynb` â†’ `Data-Preprocessing.ipynb` â†’ `Feature-Selection.ipynb` â†’ `Base-model.ipynb` â†’ `Comparison.ipynb` â†’ model-specific notebooks
    2. `Best-Model(Gradient-Boosting).ipynb` â†’ `Infererence-best-model.ipynb`
2. Alternatively, use `app.py` or `inference.py` for programmatic predictions.

## ğŸ§  What Youâ€™ll Learn

- ğŸ“Š Patterns in teen phone usage
- ğŸ” Features that predict addiction level
- ğŸ¤– Compare ML models: Logistic Regression, Random Forest, Gradient Boosting, XGBoost
- ğŸ’¾ Save & load trained models with **joblib**
- ğŸ§ª How SMOTE helps with imbalanced data

## ğŸ¯ Key Findings

- Teens using phones late at night have **higher stress levels**.
- Frequent app switching indicates **higher addiction level**.
- Gradient Boosting was the **best model** for prediction.

## ğŸ“ˆ Visualizations

- Confusion matrix heatmaps for model evaluation
- Feature importance plots
- Screen time distributions and correlation heatmaps

## ğŸ’» Deployment

- The best model is deployed using **FastAPI**
- **SwaggerUI** allows easy API interaction and testing
- Endpoints:
    - `/predict` â€“ Make predictions for new data
    - `/docs` â€“ Interactive API documentation via SwaggerUI

---

<img width="1278" height="819" alt="image" src="https://github.com/user-attachments/assets/da8c09d9-b463-4518-97e3-3fa6b105b03a" />

## ğŸ¯ Key Learnings

- Handling imbalanced datasets with **SMOTE** improves model accuracy.
- Feature selection significantly impacts model performance.
- Ensemble methods like Gradient Boosting outperform simpler models in complex classification.
- FastAPI makes ML model deployment quick and scalable.
- SwaggerUI provides a user-friendly interface for testing APIs.

---

## ğŸ“ Conclusion

This project successfully demonstrates:

- How smartphone usage can be analyzed to predict addiction risk
- The process of **data preprocessing, model training, comparison, and selection**
- Deployment of a trained model via **FastAPI** for real-time predictions

The **Gradient Boosting model** provides the best predictions and can be used to identify teens at risk of phone addiction.

---

## ğŸ”® Future Plans

- Utilize Docker to containerize this application, facilitating its convenient deployment.
- Integrate model deployment with a **front-end dashboard** for easier accessibility.
