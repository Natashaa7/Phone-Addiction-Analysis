# ğŸ“± Phone Addiction Analysis


## ğŸ“Œ Overview

This project analyzes smartphone addiction patterns in teenagers, identifying behavioral and demographic factors that contribute to addiction. The workflow involves data preprocessing, exploratory data analysis, feature selection, model comparison, and inference. The project uses multiple machine learning models to classify users into addiction risk categories and selects the best-performing model (Gradient Boosting).


## ğŸ—‚ Repository Structure 
```Phone-Addiction-Analysis/
â”‚
â”œâ”€ notebooks/
â”‚   â”œâ”€ EDA.ipynb                  # Exploratory Data Analysis
â”‚   â”œâ”€ Data-Preprocessing.ipynb   # Data cleaning and encoding
â”‚   â”œâ”€ Feature-Selection.ipynb    # Feature importance and selection
â”‚   â”œâ”€ Base-model.ipynb           # Baseline models
â”‚   â”œâ”€ Comparison.ipynb           # Model performance comparison
â”‚   â”œâ”€ Logistic-Regression.ipynb  # Logistic Regression model
â”‚   â”œâ”€ Random-Forest.ipynb        # Random Forest model
â”‚   â”œâ”€ Gradient-Boosting.ipynb    # Gradient Boosting model
â”‚   â”œâ”€ XGBoost.ipynb              # XGBoost model
â”‚   â”œâ”€ Best-Model(Gradient-Boosting).ipynb  # Retrain best model and save
â”‚   â”œâ”€ Infererence-best-model.ipynb  # Predictions using saved model
â”‚
â”œâ”€ source/model/                   # Saved ML models using joblib
â”œâ”€ teen_phone_addiction_dataset.csv
â”œâ”€ [app.py](http://app.py/)                          # Web app or API entry point
â”œâ”€ [inference.py](http://inference.py/)                    # Script for running inference
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â”œâ”€ [README.md](http://readme.md/) ```


## ğŸ§° Tools & Libraries

Programming Language: Python

Data Analysis & Visualization: Pandas, NumPy, Matplotlib, Seaborn

Machine Learning: Scikit-learn, XGBoost, Imbalanced-learn (SMOTE)

Serialization: Joblib (for saving models)

Notebook Interface: Jupyter Notebook

## ğŸ“ Workflow & Notebooks

### 1. **EDA.ipynb**

- Performs **Exploratory Data Analysis**
- Visualizes distributions, correlations, and patterns in the dataset
- Identifies trends such as high phone usage before bedtime, app usage patterns, and their effect on stress or academic performance

### 2. **Data-Preprocessing.ipynb**

- Handles **missing values**
- Encodes categorical variables
- Normalizes/standardizes features if required
- Prepares data for modeling

### 3. **Feature-Selection.ipynb**

- Computes **feature importance** using multiple methods
- Reduces dimensionality by removing low-impact features
- Ensures that the final feature set improves model performance

### 4. **Base-model.ipynb**

- Trains **baseline models** to get a performance benchmark
- Simple initial implementations of classifiers

### 5. **Comparison.ipynb**

- Implements **four ensemble methods and Logistic Regression**:
    - Logistic Regression
    - Random Forest
    - Gradient Boosting
    - XGBoost
- Compares models using metrics such as accuracy, confusion matrix, and balanced accuracy
- Visualizes performance comparison

### 6. **Logistic-Regression.ipynb, Random-Forest.ipynb, Gradient-Boosting.ipynb, XGBoost.ipynb**

- Train and evaluate each model **individually**
- Include **line-by-line explanations**
- Output model metrics and visualizations (e.g., confusion matrix heatmaps)

### 7. **Best-Model(Gradient-Boosting).ipynb**

- Chooses **Gradient Boosting** as the best performing model
- Retrains it on the full training dataset
- Saves the trained model using **joblib** for future inference

### 8. **Infererence-best-model.ipynb**

- Loads the **saved Gradient Boosting model**
- Demonstrates how to make predictions on new/unseen data
- Explains each step of inference clearly

## ğŸ’¾ Saving & Loading Models

- Models are saved in **`source/model`** using `joblib` for efficient serialization:

```python
import joblib

# Save model
joblib.dump(model, "source/model/gradient_boosting_model.joblib")

# Load model
model = joblib.load("source/model/gradient_boosting_model.joblib")

```

- Both **resampled datasets** (from SMOTE) and trained models are saved for reproducibility.

---

## ğŸš€ How to Run

1. Clone the repository:

```bash
git clone https://github.com/Natashaa7/Phone-Addiction-Analysis.git
cd Phone-Addiction-Analysis

```

1. Install dependencies:

```bash
pip install -r requirements.txt

```

1. Run notebooks in this order for full workflow:
    1. `EDA.ipynb` â†’ `Data-Preprocessing.ipynb` â†’ `Feature-Selection.ipynb` â†’ `Base-model.ipynb` â†’ `Comparison.ipynb` â†’ model-specific notebooks
    2. `Best-Model(Gradient-Boosting).ipynb` â†’ `Infererence-best-model.ipynb`
2. Alternatively, use `app.py` or `inference.py` for programmatic predictions.

## ğŸ§  What Youâ€™ll Learn

- ğŸ“Š Patterns in teen phone usage
- ğŸ” Features that predict addiction level
- ğŸ¤– Compare ML models: Logistic Regression, Random Forest, Gradient Boosting, XGBoost
- ğŸ’¾ Save & load trained models with **joblib**
- ğŸ§ª How SMOTE helps with imbalanced data

## ğŸ¯ Key Findings

- Teens using phones late at night have **higher stress levels**
- Frequent app switching indicates **higher addiction level**
- Gradient Boosting was the **best model** for prediction

## ğŸ“ˆ Visualizations

- Confusion matrix heatmaps for model evaluation
- Feature importance plots
- Screen time distributions and correlation heatmaps

![Workflow](docs/jupyter_workflow.gif) 
