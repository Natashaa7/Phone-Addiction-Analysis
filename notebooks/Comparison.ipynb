{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5441a5b4-3a38-4bae-ab51-b536df69a95d",
   "metadata": {},
   "source": [
    "# Comparison between models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46abb1db-acd2-4353-8fbd-5e5424a38cf3",
   "metadata": {},
   "source": [
    "## 1. High-level observations\n",
    "\n",
    "All models perform nearly perfect on training (especially tree-based ones â†’ clear sign of overfitting).\n",
    "\n",
    "On test data, performance differs mainly in how well they handle:\n",
    "\n",
    "- Minority class (0.0, only 3 samples)\n",
    "\n",
    "- Mid-size class (1.0, 86 samples)\n",
    "\n",
    "- Majority class (2.0, 511 samples)\n",
    "\n",
    "Balanced Accuracy is crucial here because raw accuracy is misleading (predicting class 2.0 alone gives ~85% already).\n",
    "\n",
    "## 2. Model-by-model analysis\n",
    "### ðŸ”¹ Random Forest\n",
    "\n",
    "- Test Accuracy: 92.5%\n",
    "\n",
    "- Balanced Accuracy: 0.55 (very poor)\n",
    "\n",
    "Confusion Matrix: misclassifies class 0 heavily (only 1/3 correct), struggles more on class 1.\n",
    "\n",
    "#### Why:\n",
    "\n",
    "- Random Forests are ensembles of deep trees â†’ they can overfit training (shown by perfect scores).\n",
    "\n",
    "- With imbalanced data, they bias toward the majority class, since each treeâ€™s splitting criterion optimizes overall accuracy, not minority class recall.\n",
    "\n",
    "- Thus, while overall accuracy looks okay, it fails on balanced accuracy.\n",
    "\n",
    "### ðŸ”¹ XGBoost\n",
    "\n",
    "- Test Accuracy: 95.2%\n",
    "\n",
    "- Balanced Accuracy: 0.83\n",
    "\n",
    "Much better recall on class 1 (86% vs RFâ€™s 87%) and stable on class 2.\n",
    "\n",
    "Slightly struggles with the tiny class 0 (2/3 correct).\n",
    "\n",
    "#### Why:\n",
    "\n",
    "- XGBoost uses boosting â†’ it iteratively focuses on mistakes, so minority and medium-sized classes get more attention than in RF.\n",
    "\n",
    "- Regularization (shrinkage, tree depth limits) prevents extreme overfitting compared to RF.\n",
    "\n",
    "- Overall, this makes it more robust to imbalance.\n",
    "\n",
    "### ðŸ”¹ Gradient Boosting\n",
    "\n",
    "- Test Accuracy: 98%\n",
    "\n",
    "- Balanced Accuracy: 0.87 (best among all).\n",
    "\n",
    "Predicts class 0 (2/3 correct) and does excellent on class 1 (95% recall).\n",
    "\n",
    "#### Why:\n",
    "\n",
    "- Similar to XGBoost, but often more conservative â†’ fits more smoothly with higher bias but less variance.\n",
    "\n",
    "- This prevents overfitting on minority data while still handling imbalance effectively.\n",
    "\n",
    "- The very low log-loss (0.046) shows strong confidence calibration, meaning its probability outputs are very well aligned with reality.\n",
    "\n",
    "### ðŸ”¹ Logistic Regression\n",
    "\n",
    "- Test Accuracy: 98% (same as GBM)\n",
    "\n",
    "- Balanced Accuracy: 0.87 (also excellent).\n",
    "\n",
    "Confusion Matrix: also 2/3 correct on class 0, great recall on 1 and 2.\n",
    "\n",
    "#### Why:\n",
    "\n",
    "- Logistic regression is a linear model. If your features are already linearly separable (or nearly so), LR can do very well.\n",
    "\n",
    "- Unlike trees, it doesnâ€™t overfit easily â†’ you see stable performance across train/test.\n",
    "\n",
    "- Its performance here suggests your dataset has features that linearly separate classes quite well, especially between 1 and 2.\n",
    "\n",
    "## 3. Why one performs better than another\n",
    "\n",
    "- Random Forest < Boosted Trees < Logistic Regression/Gradient Boosting\n",
    "\n",
    "- Random Forest overfits and struggles with imbalance â†’ weak generalization.\n",
    "\n",
    "- XGBoost corrects some of this by focusing on errors, but still slightly worse calibration.\n",
    "\n",
    "- Gradient Boosting and Logistic Regression both achieve strong generalization and balanced accuracy, suggesting the data is not too complex â€” linear or weak learners suffice.\n",
    "\n",
    "- Logistic Regressionâ€™s competitiveness shows non-linear power isnâ€™t always necessary if feature engineering is strong.\n",
    "\n",
    "- Gradient Boosting edges out XGBoost because it regularizes a bit more, leading to smoother probability estimates.\n",
    "\n",
    "## 4. Beyond metrics\n",
    "\n",
    "- Interpretability: Logistic Regression is simplest to explain (feature weights â†’ direct interpretation).\n",
    "\n",
    "- Scalability: Random Forest and Gradient Boosting are heavier; Logistic Regression is very lightweight.\n",
    "\n",
    "- Calibration: Gradient Boosting had the lowest log loss â†’ better probability estimates.\n",
    "\n",
    "- Imbalanced Data: Boosting methods handle imbalance better than bagging (RF). Logistic Regression naturally balances better if features separate well.\n",
    "\n",
    "## âœ… Conclusion:\n",
    "\n",
    "- Random Forest â†’ not suitable for your imbalanced setup (biased toward majority).\n",
    "\n",
    "- XGBoost â†’ strong, but slightly less calibrated.\n",
    "\n",
    "- Gradient Boosting â†’ best overall balance of recall, calibration, and accuracy.\n",
    "\n",
    "- Logistic Regression â†’ surprisingly effective; if interpretability matters, this is the best choice.\n",
    "\n",
    "ðŸ‘‰ If your goal is maximum balanced accuracy: Gradient Boosting.\n",
    "ðŸ‘‰ If your goal is simplicity + interpretability: Logistic Regression.\n",
    "ðŸ‘‰ If your goal is scalable ensemble: XGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
